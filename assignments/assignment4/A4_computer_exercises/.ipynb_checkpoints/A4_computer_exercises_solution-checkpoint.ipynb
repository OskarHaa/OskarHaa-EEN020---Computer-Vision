{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2165ed38",
   "metadata": {},
   "source": [
    "# Computer Vision, Assignment 4: Model Fitting and Local Optimization\n",
    "\n",
    "In this assignment, you will how to robustly estimate camera parameters and how to jointly optimize the scene geometry. \n",
    "You will mainly use RANSAC and Bundle Adjustment.\n",
    "\n",
    "Please see Canvas for detailed instructions on what is expected for a passing/higher grade.\n",
    "All computer exercises that are not marked **OPTIONAL** are \"mandatory\" in the sense described on Canvas.\n",
    "\n",
    "\n",
    "### Submission Requirements:\n",
    "Your final lab submission should include:\n",
    "1. Your edited **notebook file** (`.ipynb`).\n",
    "2. An **HTML printout** of the executed notebook with all outputs visible: File → Save and export Notebook As → HTML\n",
    "3. A **pdf report** containing answers to the theoretical exercises (see separate document).\n",
    "\n",
    "\n",
    "### Reusing functions from prior assignment:\n",
    "In this assignment, you will build on your prior work. Many of the exercises require you to use functions implemented in assignment 3. To do so most easily, we recommend that you paste those functions into `helpers.py` and import them here by e.g. `from helpers import estimate_F_DLT`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2573ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for creating responsive plots\n",
    "%matplotlib widget  \n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# Note: These functions are provided for your convenience, use them where needed\n",
    "from supplied import pflat, plot_camera, rital"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063c5283",
   "metadata": {},
   "source": [
    "# Estimate Essential Matrix using RANSAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640d7a65-50e6-4f5c-9a7b-c508a00286fd",
   "metadata": {},
   "source": [
    "#### *Theoretical exercises 1-2* (see pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46690e17-96cc-418d-b6f4-47977a8d7554",
   "metadata": {},
   "source": [
    "## Computer Exercise 1\n",
    "<figure align=\"center\">\n",
    "    <img alt=\"left\" src=\"data/round_church1.jpg\" width=\"250px\">\n",
    "&nbsp; &nbsp; &nbsp; &nbsp;\n",
    "    <img alt=\"right\" src=\"data/round_church2.jpg\" width=\"250px\">\n",
    "    <img alt=\"right\" src=\"data/round_church_recon.png\" width=\"250px\">\n",
    "    <figcaption>Figure 1: Two images of a church and an example of the obtained 3D reconstruction.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5743a8f2-59e8-448f-930a-f04f1e1e8220",
   "metadata": {},
   "source": [
    "The goal of this exercise is to robustly estimate essential matrix $E$ from point matches between the two images. For this, we will use RANSAC with an eight point solver.\n",
    "\n",
    "The data provided contains calibration matrix $K$, which is the same for both images, and a cell $\\mathbf{x}$ with matched image points for the two images. Note that $\\mathbf{x}$ are noisy and contain some fraction of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "13871cbe-3efe-4c9b-ad39-e80caaf47a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we supply some preliminaries, just loading images and data\n",
    "img1 = plt.imread('./data/round_church1.jpg')\n",
    "img2 = plt.imread('./data/round_church2.jpg')\n",
    "\n",
    "data = sp.io.loadmat('./data/compEx1data.mat')\n",
    "K = data[\"K\"]\n",
    "x = [x.astype(np.float32) for x in data['x'][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8325fdb2-f19c-4b75-908c-b4fc0720ee0a",
   "metadata": {},
   "source": [
    "### Task 1.1\n",
    "\n",
    "First, compute the essential matrix $E$ with an eight point algorithm using all the point correspondences. Remember to normalize image points using $K$ beforehand. Then convert it to a fundamental matrix $F$. Recall that in Assignment 3, you created functions `estimate_F_DLT(x1s, x2s)`, `enforce_essential(E_approx)`, and `convert_E_to_F(E,K1,K2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52031163-ea0a-4fe3-aa02-997ebbfcb9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional imports\n",
    "from helpers import estimate_F_DLT, enforce_essential, convert_E_to_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239110cf-bf4d-4aee-8018-05c80cb95f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Your code here ------\n",
    "\n",
    "# Insert code where you compute the essential matrix\n",
    "\n",
    "# Insert code where you transform the essential matrix to a fundamental matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aa30ee-2cb9-4a10-97d5-b53a4cdfc5b3",
   "metadata": {},
   "source": [
    "### Task 1.2 \n",
    "\n",
    "Compute the epipolar lines $\\mathbf{l}_2 = F\\mathbf{x}_1$ and $\\mathbf{l}_1 = F^T\\mathbf{x}_2$. Compute the RMS distance between the image points ($\\mathbf{x}_1$ and $\\mathbf{x}_2$) and corresponding epipolar lines ($\\mathbf{l}_1$ and $\\mathbf{l}_2$, respectively). \n",
    "\n",
    "\\begin{align}\n",
    "& e_{RMS} = \\sqrt{\\frac{1}{2m}\n",
    "\\left(\n",
    "\\sum_{i=1}^{m} d^2({\\mathbf{x}_1}_i,{\\mathbf{l}_1}_i) + \n",
    "\\sum_{i=1}^{m} d^2({\\mathbf{x}_2}_i,{\\mathbf{l}_2}_i)\n",
    "\\right)\n",
    "},\\nonumber \\\\\n",
    "& \\text{where }~d(\\mathbf{x}, \\mathbf{l}) = \\frac{|l_{1}x_{1}+l_{2}x_{2}+l_{3}|}{\\sqrt{l_{1}^2+l_{2}^2}}.\n",
    "\\end{align}\n",
    "\n",
    "In Assignment 3, you implemented the distances $d(\\mathbf{x}, \\mathbf{l})$ from image points to the corresponding epipolar lines for the second image in the function `compute_epipolar_errors(F, x1s, x2s)`.\n",
    "\n",
    "\n",
    "**HINT:** If you transpose the fundamental matrix and change the order of the correpondences, i.e. run `compute_epipolar_errors(F.T, x2s, x1s)` you can obtain the errors to the epipolar lines in the first image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce0b438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional imports\n",
    "from helpers import compute_epipolar_errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04c4577-8432-474b-b528-04a37f567c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Your code here ------\n",
    "\n",
    "# Compute the epipolar lines, l1 and l2 \n",
    "\n",
    "# Compute the RMS distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f97e953-37c6-4fbe-ae81-a1a313a6716c",
   "metadata": {},
   "source": [
    "### Task 1.3\n",
    "\n",
    "Plot two separate histograms with 100 bins for the epipolar errors $\\{d({\\mathbf{x}_1}_i,{\\mathbf{l}_1}_i)\\}$ in the first image and $\\{d({\\mathbf{x}_2}_i,{\\mathbf{l}_2}_i)\\}$ in the second image, respectively. See for example your solution to Assignment 3 Computer Exercise 2.2 for some code on plotting histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3985db-2657-44e6-aad5-1dea7180aa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Your code here ------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d82db72-7885-49c2-a824-ca54c50e62a6",
   "metadata": {},
   "source": [
    "### Task 1.4\n",
    "\n",
    "Pick 20 points in the first image at random and plot these in the same figure as the image. Also plot the corresponding epipolar lines in the same image using the function `rital`. Repeat the plot for the second image.\n",
    "\n",
    "Q: **Do the plots look reasonable, are points close to the epipolar lines? If not, what do you think could be the cause of the bad estimate?** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e47533",
   "metadata": {},
   "source": [
    "A: [Insert your answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d74a51-4520-486e-b9ad-9b408e9bb67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Your code here ------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3adf1d-413a-4aa6-b65d-30e0beae4c5b",
   "metadata": {},
   "source": [
    "### Task 1.5\n",
    "\n",
    "Next, use RANSAC to robustly compute $E$. Create a function `estimate_E_robust(x1, x2, eps)` that does it, where `eps` is the inlier threshold. Remember again to normalize `x1` and `x2` using $K$ beforehand. Note that for this problem of estimating $E$, a point correspondence $({\\mathbf{x}_1}_i, {\\mathbf{x}_2}_i)$ is a single measurement. Use the same eight point solver as above.\n",
    "\n",
    "Note that this time only a subset of correspondences will be used by this solver (since in the RANSAC loop, you first randomly sample a subset of correspondences and then compute candidate $E$ from this subset).\n",
    "\n",
    "Finally, to make RANSAC work, you need to implement an error function that measures how far a single measurement ($({\\mathbf{x}_1}_i, {\\mathbf{x}_2}_i)$) is to the candidate model ($E$). Use the following error function: $\\frac12 \\left ( d^2({\\mathbf{x}_1}_i,{\\mathbf{l}_1}_i) + d^2({\\mathbf{x}_2}_i,{\\mathbf{l}_2}_i)\\right )$, where the epipolar lines ${\\mathbf{l}_j}_i$ are computed as before but using $E$ (and therefore are normalized). This error should be familiar to you based on the previous exercises, reuse your implementation from there!\n",
    "\n",
    "A suggested inlier threshold is 2px (which should also be normalized, see function description below). \n",
    "\n",
    "Q: **How many inliers did you get for the returned solution of $E$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ed782f",
   "metadata": {},
   "source": [
    "A: [Insert your answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18de42f-5eb1-4a75-a713-9f8cc8d2112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_E_robust(x1, x2, eps, seed=None):\n",
    "    \"\"\"\n",
    "    RANSAC estimate of essential matrix using normalized correspondences x1 and x2 and a normalized threshold.\n",
    "    Note: Make sure to normalize things before using it in this function!\n",
    "    -------------------------------------------\n",
    "    x1: Normalized keypoints in image 1 - 3xN np.array or 2xN np.array, as you desire \n",
    "    x2: Normalized keypoints in image 2 - 3xN np.array or 2xN np.array, as you desire \n",
    "    eps: Normalized inlier threshold - float\n",
    "\n",
    "    Returns:\n",
    "    E: 3x3 essential matrix\n",
    "    inliers: The inlier points\n",
    "    errs: The epipolar errors\n",
    "    iters: How many iterations it took\n",
    "    \"\"\"\n",
    "    # TIPS: \n",
    "    # * You can use the already created functions, enforce_essential, estimate_F_DLT, and compute_epipolar_errors\n",
    "    # * Normalizing the pixel threshold can be done by e.g. eps = threshold_px / K[0,0]\n",
    "    # * To create an estimate for E using DLT for a random subset of calibrated correspondences...\n",
    "    # ...you can chain your functions like: E = enforce_essential(estimate_F_DLT(x1[:, randind], x2[:, randind]))\n",
    "\n",
    "    # * Pseudo code for computing inliers:\n",
    "    # e1 = compute_epipolar_errors(E, x1, x2)**2 \n",
    "    # e2 = compute_epipolar_errors(E.T, x2, x1)**2\n",
    "    # inliers = (1/2)*(e1+e2) < eps**2\n",
    "    \n",
    "    # ------ Your code here ------\n",
    "    return E, inliers, errs, iters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5520594d-999d-4b10-bb27-2c7c572e08d6",
   "metadata": {},
   "source": [
    "### Task 1.6\n",
    "\n",
    "Again, compute the RMS distance between the image points and corresponding estimated epipolar lines in both images. Also plot a histogram of epipolar errors for both images as before. \n",
    "\n",
    "Q: **Which is the better estimate of the essential matrix, and why?** Repeat plotting 20 random points with the corresponding epipolar lines, but this time pick random points from the subset of correspondences that are ***inliers***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f92022",
   "metadata": {},
   "source": [
    "A: [Insert your answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81745652-5cd2-4fcf-81eb-5571efeacda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Your code here ------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd515d5-9fb4-4702-87b4-202d04348065",
   "metadata": {},
   "source": [
    "## Computer Exercise 2\n",
    "<figure align=\"center\">\n",
    "    <img alt=\"left\" src=\"data/fountain1.png\" width=\"250px\">\n",
    "&nbsp; &nbsp; &nbsp; &nbsp;\n",
    "    <img alt=\"right\" src=\"data/fountain2.png\" width=\"250px\">\n",
    "    <img alt=\"right\" src=\"data/fountain_recon.png\" width=\"250px\">\n",
    "    <figcaption>Figure 2: Two images of a fountain and an example of the obtained 3D reconstruction.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352abf4e-fa4f-44a9-bab6-6853fb5d17a9",
   "metadata": {},
   "source": [
    "In this exercise you will build a 2-view reconstruction pipeline that connects feature extraction,\n",
    "matching, robust essential matrix estimation from the previous computer exercise, and triangulation.\n",
    "\n",
    "You can use the two supplied images from the data and the intrinsics provided. We load the required data for your convenience below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "40b9beb1-49ce-4dcd-8ca0-75fdc33f312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sp.io.loadmat('./data/compEx2data.mat')\n",
    "K = data['K']\n",
    "img1 = (plt.imread('./data/fountain1.png') * 255).astype('uint8')\n",
    "img2 = (plt.imread('./data/fountain2.png') * 255).astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4ca979-c966-4ede-9f01-ca425a1d8b76",
   "metadata": {},
   "source": [
    "### Task 2.1\n",
    "\n",
    "We will begin by computing SIFT features and matching them. Since OpenCV has quite a difficult API for this we have supplied some code snippets. And you already learned much of the basics of this from Assignment 2, so we provide some basic code to get you started.\n",
    "\n",
    "The implementation is based on their official guide for image matching (https://docs.opencv.org/3.4/dc/dc3/tutorial_py_matcher.html) and uses Lowe's ratio test to filter out matches that are too similar, this should be similar to what you have done in previous assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "58f7ca4d-4202-4b9c-8d1d-5938ec2dd130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some supplied code on how to compute matches using OpenCV\n",
    "\n",
    "from cv2 import SIFT_create, cvtColor, COLOR_RGB2GRAY, BFMatcher, drawMatchesKnn\n",
    "rgb2gray = lambda img: cvtColor(img, COLOR_RGB2GRAY)\n",
    "sift = SIFT_create(contrastThreshold=0.02, edgeThreshold=10, nOctaveLayers=3)\n",
    "\n",
    "# We detect keypoints and simultaneously describe them using SIFT\n",
    "kp1, des1 = sift.detectAndCompute(rgb2gray(img1),None)\n",
    "kp2, des2 = sift.detectAndCompute(rgb2gray(img2),None)\n",
    "\n",
    "# We use a k-NN-like system to find the most similar descriptions\n",
    "all_matches = FlannBasedMatcher().knnMatch(des1, des2, k=2)\n",
    "# Apply ratio test\n",
    "# Here we filter out matches that are too similar to other matches (because then they are likely wrong)\n",
    "# This is standard in OpenCV, see https://docs.opencv.org/4.x/dc/dc3/tutorial_py_matcher.html\n",
    "matches = []\n",
    "for m,n in all_matches:\n",
    "    if m.distance < 0.75*n.distance:\n",
    "        matches.append([m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cfa73f-d4ab-4a54-bcbd-68d60938e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code for a simple plot of the filtered matches\n",
    "\n",
    "# Here is some supplied code from https://docs.opencv.org/4.x/dc/dc3/tutorial_py_matcher.html\n",
    "# Feel free to play around with it\n",
    "img3 = drawMatchesKnn(img1,kp1,img2,kp2,good_matches,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(img3,),plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6633b95a-5e0b-4757-a6f9-f4986cddc22c",
   "metadata": {},
   "source": [
    "Q: **Run the code above. Do the matches look reasonable?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e65f8f0-5590-4543-84af-5211a31279d9",
   "metadata": {},
   "source": [
    "A: [Insert your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626d01e2-0348-4857-b7a3-40174b0286a7",
   "metadata": {},
   "source": [
    "Q: **How many SIFT features did you find for the two images, respectively? How many total matches did you find? How many good matches did you find after the ratio test?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca4aed8",
   "metadata": {},
   "source": [
    "A: [Insert your answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6551789-482f-4302-9005-787229e30ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Your code here ------\n",
    "\n",
    "# Count SIFT features per image\n",
    "\n",
    "# Count number of total matches\n",
    "\n",
    "# Count the number of good matches after the ratio-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dbd225-4e99-45ca-9943-6fc6e9303a40",
   "metadata": {},
   "source": [
    "Now we supply a little more code just to make it easier. We extract the keypoints corresponding to the good matches and save them as x1 and x2. We do this because currently the type of kp1 etc... are OpenCV specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d91cca3c-da5c-4a86-baa5-585e8215f90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplied code for extracting numpy arrays from matching keypoints\n",
    "matches_np = np.array([[m[0].queryIdx, m[0].trainIdx] for m in matches]).T\n",
    "xs = [f1[:, matches_np[0,:]], f2[:2, matches_np[1,:]]]\n",
    "x1, x2 = xs[0], xs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204cfb62-6081-4aa1-8594-2c52c0aae0bd",
   "metadata": {},
   "source": [
    "### Task 2.2\n",
    "\n",
    "Now you should find the essential matrix describing the transformation between the two images.\n",
    "Because not all matches are correct, you need to use RANSAC to find a set of good correspondences (inliers). \n",
    "To estimate the essential matrix use the function `estimate_E_robust(x1,x2,eps)` that you created in the previous computer exercise.\n",
    "\n",
    "Q: **How many inliers did you find?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33913db6",
   "metadata": {},
   "source": [
    "A: [Insert your answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cc5770-4709-431b-ae21-4ff9bccacf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Your code here ------\n",
    "\n",
    "# Compute the essential matrix based on the keypoint matches we just computed between the two images\n",
    "\n",
    "# Print the number of inliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d7ad53-f1c1-43d7-9d95-4dbe3727967e",
   "metadata": {},
   "source": [
    "### Task 2.3\n",
    "\n",
    "After getting the robust essential matrix estimation, you should find the camera matrix of the second view. Remember that there are 4 possible solutions (see Theoretical Exercise 7 of HA3)! You should pick the solution that has more points in front of the camera. Remember, in Assigment 3 you implemented `extract_P_from_E(E)`\n",
    "\n",
    "**Hint:** You need to perform triangulation by using a function from a previous computer exercise. The 3D points X and the 2d correspondences x1 and x2 wil also be needed for the next task in this assignment!\n",
    "\n",
    "Q: **Which of the solutions seems correct to you?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1292bd3f",
   "metadata": {},
   "source": [
    "A: [Insert your answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876ff11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional import\n",
    "from helpers import extract_P_from_E, triangulate_3D_point_DLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c9639-c54b-45db-befd-d2b35290a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Your code here ------\n",
    "\n",
    "# Compute the 4 possible camera matrices\n",
    "\n",
    "# Plot all (4) of them together with the triangulated 3D points X!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5afeeae-ed63-4b00-921b-cb819b4ecde3",
   "metadata": {},
   "source": [
    "### Summary of reconstruction pipeline\n",
    "\n",
    "In summary, you have performed a 2d-view reconstruction pipeline consisting of the following steps:\n",
    "1. **Load the two images**, find SIFT features, and match them.\n",
    "\n",
    "2. **Estimate the essential matrix** robustly using  \n",
    "   `estimate_E_robust(x1, x2, eps)`.\n",
    "\n",
    "3. **Compute the four possible camera matrix pairs** for the essential matrix, and for each pair:\n",
    "   1. Triangulate the 3D points using the camera matrix pair and the matched image points.\n",
    "   2. Compute the camera centers and principal directions of both cameras.\n",
    "   3. Plot everything in 3D.\n",
    "\n",
    "4. **Choose the correct solution** (out of the four) by selecting the one that yields the highest number of 3D points (inliers) lying in front of the cameras.  \n",
    "\n",
    "The final visualization should resemble the right-most result in the first Figure of Computer Exercise 2 after removing most outliers.\n",
    "\n",
    "This pipeline will come in handy for the project and somewhat resembles the first stage in any modern SfM pipeline like [COLMAP](https://github.com/colmap/colmap)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d094a66-4240-45c3-973b-bfc27158bab7",
   "metadata": {},
   "source": [
    "#### *Theoretical exercises 3* (see pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0bbdc5-57f0-4712-ab7b-ded0f4615e4f",
   "metadata": {},
   "source": [
    "## Computer Exercise 3\n",
    "\n",
    "In this exercise you will use the solution from Computer Exercise 2 as a starting point and locally improve it using the Levenberg-Marquardt method. If you have doubts about the correctness of your solution from the earlier exercise, you can instead use the provided data in `compEx3data.mat` as starting solution (you can load it like `data = sp.io.loadmat('./data/compEx3data.mat')` and then use that for your x and camera matrices)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65934b0c-bf9e-4a2c-a983-1637bc29a226",
   "metadata": {},
   "source": [
    "The goal is to refine the solution **3D points (not camera matrices, those you can assume fixed**).\n",
    "You will loop over the 3D points and update them one by one using Levenberg-Marquardt (LM), please see the lecture notes or [Wikipedia page on Levenberg–Marquardt](https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm) for more information.\n",
    "\n",
    "In short, LM is an iterative method for minimizing a non-linear least squares objective\n",
    "\\begin{equation}\n",
    "   F(v)=\\|r(v)\\|^2, \n",
    "\\end{equation}\n",
    "with respect to $v$.\n",
    "In LM, the update is given by\n",
    "\\begin{equation}\n",
    "\\delta v =  - (J(v)^TJ(v)+\\mu I)^{-1}J(v)^Tr(v),\n",
    "\\end{equation}\n",
    "where $J(v)$ is the Jacobian of $r(v)$ at $v$.\n",
    "Here $\\mu>0$ is a damping factor that is adjusted adaptively and $v$ is the previous solution.\n",
    "The new solution is $v+\\delta v$ and we iterate until convergence (or for a fixed number of iterations).\n",
    "\n",
    "We can also define the **reprojection error** to be: $\\sum_{i=1}^m \\left\\|r_{i}(X_j) \\right\\|^2 $ (as in Equation (2) in the Theoretical Exercises)\n",
    "\n",
    "In our setting, we have $v=X_j$ and $r(v)=r(X_j)$. Please see the main PDF for some more details about the definition of $r$ (e.g. Equation (3) for the definition)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8035b3-2251-4142-8f8b-b6b3beacfa6d",
   "metadata": {},
   "source": [
    "You will have to implement three functions in this exercise:\n",
    "\n",
    "1. `(err,res) = compute_reprojection_error(P_1,P_2,X_j,x_1j,x_2j)`: a function that computes the reprojection error ($\\sum_{i=1}^m \\left\\|r_{i}(X_j) \\right\\|^2 $) given two cameras $P_i$, a 3D point $X_j$ and 2D points $x_{ij}$ in the images corresponding to the cameras $P_i$ and the 3D point $X_j$. It also returns the values of all the individual residuals as a second output, i.e., the residual vector $r(\\mathbf{X}_j)$.\n",
    "    \n",
    "2. `(r,J) = linearize_reproj_err(P_1,P_2,X_j,x_1j,x_2j)`: a function that computes the linearization ($r$ and $J$) as described in Equation (7) in the Theoretical Exercises.\n",
    "    \n",
    "3. `delta_X_j = compute_update(r, J, mu)`: a function that computes the LM-update given $r$, $J$ and $\\mu$ as described in Equation (9) in the Theoretical Exercises.\n",
    "\n",
    "The idea is now to use *decoupled* LM, i.e. loop over the 3D points and refine them one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa79fb6-4fe6-43d9-9982-4adc8b8fe7ae",
   "metadata": {},
   "source": [
    "Let's first start by implementing the functions in code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc33fc3-48b1-4cd4-a32d-529a43931661",
   "metadata": {},
   "source": [
    "**NOTE**: Below we have provided suggestion for the function signatures. However, you are free to implement the actual functions however you'd like. For example, you don't have to take the inputs and outputs on the exact form we suggest. This is one way to store the Jacobians, residuals, etc..., feel free to decide how you feel is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249fa49-16d4-4130-bacb-9eb49283caec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random useful commands:\n",
    "\n",
    "# Computes the LM update .\n",
    "# C = J.T @ J + mu * np.eye(J.shape[1])\n",
    "# c = J.T @ r\n",
    "# delta = -np.linalg.solve(C, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a488c69f-abbb-48df-9748-b9a760e8b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reprojection_error(P_1, P_2, X_j, x_1j, x_2j):\n",
    "    \"\"\"\n",
    "    Compute the reprojection error for a single 3D point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P_1, P_2 : ndarray (3, 4)\n",
    "        Projection matrices.\n",
    "\n",
    "    X_j : ndarray (4,)\n",
    "        3D homogeneous point.\n",
    "\n",
    "    x_1j, x_2j : ndarray (2,)\n",
    "        Observed image positions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    err : float \n",
    "        Reprojection error.\n",
    "\n",
    "    r : Residual vector. For example, an ndarray of shape (2, 2)\n",
    "        However, you can decide in your own implementation.\n",
    "    \"\"\"\n",
    "    # ------ Your code here ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e1b83-5881-48db-ab6b-353b4451b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearize_reproj_err(P_1, P_2, X_j, x_1j, x_2j):\n",
    "    \"\"\"\n",
    "    Linearize the reprojection error for a single 3D point observed in two views.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P_1, P_2 : ndarray (3, 4)\n",
    "        Camera matrices.\n",
    "\n",
    "    X_j : ndarray (4,)\n",
    "        Homogeneous 3D point.\n",
    "\n",
    "    x_1j, x_2j : ndarray (2,)\n",
    "        Observed image coordinates for this point in cameras 1 and 2.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    r : Linearization of r. For example an ndarray of shape (4,) representing concatenated reprojection residuals: [r1_x, r1_y, r2_x, r2_y].\n",
    "        However, you can decide for yourself how you want to store this.\n",
    "\n",
    "    J : Linearization of J. For example, an ndarray of shape (4, 4) representing a concatenation of the two Jacobians\n",
    "        However, you can decide for yourself how you want to store this.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This creates the linear system used in Levenberg-Marquardt.\n",
    "    \"\"\"\n",
    "    # ------ Your code here ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb75ad8c-b70a-4353-a2f9-77c349ef5989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_update(r, J, mu):\n",
    "    \"\"\"\n",
    "    Compute the LM update step for a single 3D point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    r : For example, ndarray of shape (4,)\n",
    "        Residual vector.\n",
    "\n",
    "    J : For example, ndarray of shape (4, 4)\n",
    "        Jacobian of the residuals w.r.t. the 3D point.\n",
    "\n",
    "    mu : float\n",
    "        Levenberg–Marquardt damping parameter.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    delta : ndarray of shape (4,)\n",
    "        Update ΔX to apply to the 3D point.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Solves:\n",
    "        (JᵀJ + μI) δ = -Jᵀ r\n",
    "    \"\"\"\n",
    "    # ------ Your code here ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2419d930-da5a-4c7b-bc0c-04d2201d6984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Your code here ------\n",
    "\n",
    "# Print the median reprojection error before bundle adjustment\n",
    "\n",
    "# Perform bundle adjustment by looping through your N points\n",
    "# For each point, compute the Levenberg-Marquadt update\n",
    "# Use the update to update the 3D point (the camera matrices are fixed!)\n",
    "\n",
    "# Print the median reprojection error after bundle adjustment\n",
    "\n",
    "# Plot the refined 3D points in the same plot as the originals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc2a082-6e80-4797-bed0-f32a64f191f3",
   "metadata": {},
   "source": [
    "Compare the total reprojection error (sum of $\\sum_{i=1}^m \\left\\|r_{i}(X_j) \\right\\|^2 $ over all 3D points) before and after running LM.\n",
    "Also compare the median reprojection error (median of $\\sum_{i=1}^m \\left\\|r_{i}(X_j) \\right\\|^2 $ over all 3D points) before and after running LM.\n",
    "\n",
    "Finally plot the refined 3D points in the same plot as the originals.\n",
    "\n",
    "Q: **What do you observe?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e13d07",
   "metadata": {},
   "source": [
    "A: [Insert your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc73735c-cb50-444e-b6f3-40bd72cd9b55",
   "metadata": {},
   "source": [
    "## **OPTIONAL**: Computer Exercise 4 (10 points)\n",
    "\n",
    "Perform an empirical noise sensitivity analysis of your LM-solver from the previous exercise.\n",
    "Add i.i.d. mean-zero Gaussian noise with standard deviation $\\sigma_X\\in\\{0,0.1\\mathrm{m}\\}$ to the 3D points and $\\sigma_x\\in\\{0,3\\mathrm{px}\\}$ to the 2D points (from SIFT), yielding (at least) four noise combinations ($\\sigma_X$, $\\sigma_x$) to try.\n",
    "See how the total reprojection error and median reprojection error as computed in the previous exercise varies before and after LM with the added noise.\n",
    "If you want to, you can test other noise types as well.\n",
    "\n",
    "Q: **Report your findings with plots and numbers in some reasonable manner.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd34f3",
   "metadata": {},
   "source": [
    "A: [Insert your answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c5a1a-24f2-40d4-be2b-5cb2fdd09d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Your code here ------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (main)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
